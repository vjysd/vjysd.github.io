<!DOCTYPE HTML>
<html lang="en">
<head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <!-- Google tag (gtag.js) -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-0J9NX0WVWS"></script>
  <script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-0J9NX0WVWS');
  </script>
  
  <title>Vijay Sadashivaiah</title>
  <meta name="author" content="Vijay Sadashivaiah">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" type="image/png" href="images/avatar.png">
</head>

<body>
  <!-- Google Tag Manager (noscript) -->
  <noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-TTKGWRX"
  height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
  <!-- End Google Tag Manager (noscript) -->

  <table style="width:100%;max-width:1000px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Vijay Sadashivaiah</name>
              </p>
              <p>
                I am a PhD candidate in Computer Science at the <a href="https://science.rpi.edu/computer-science" target="_blank">Rensselaer Polytechnic Institute</a> advised by <a href="https://faculty.rpi.edu/james-hendler" target="_blank">Professor James A. Hendler </a> and <a href="https://dial.rpi.edu/people/pingkun-yan" target="_blank">Professor Pingkun Yan</a>. 
                My research interests lie in improving the interpretability of computer vision and multimodal neural networks (text, images, point-based, etc.) and foundation models in critical applications. I'm generally interested in computer vision, deep learning, generative AI, and Explainable AI.
              </p>

              </p>
                Previously, I received my B.S. in Electrical Engineering at <a href="https://www.pes.edu/" target="_blank">PES Institute of Technology</a>, a M.S. in Computer Science at <a href="https://science.rpi.edu/computer-science" target="_blank">Rensselaer Polytechnic Institute</a> and a M.S. in Biomedical Engineering at <a href="https://www.jhu.edu/" target="_blank">Johns Hopkins University.</a>
                I have been advised by such wonderful advisors: <a href="https://www.bme.jhu.edu/people/faculty/sridevi-v-sarma/" target="_blank">Professor Sridevi Sarma</a> at <a href="https://www.jhu.edu/" target="_blank">JHU</a>, <a href="https://scholar.google.com/citations?user=s22hIQ0AAAAJ&hl=en" target="_blank">Dr. Qiang Chen</a> and <a href="https://www.libd.org/team/kristen-maynard-phd/" target="_blank">Dr. Kristen Maynard</a> at <a href="http://libd.org" target="_blank">LIBD</a>, <a href="https://scholar.google.com/citations?user=rej0aokAAAAJ&hl=en" target="_blank">Professor Carl Petersen</a> at <a href="http://www.epfl.ch/" target="_blank">EPFL</a>, and <a href="https://www.ee.ucla.edu/achuta-kadambi/" target="_blank">Professor Achuta Kadambi</a> at <a href="http://www.epfl.ch/" target="_blank">MIT</a>.
              </p>
              <p style="text-align:center">
                <a href="mailto:sadasv2@rpi.edu">Email</a> &nbsp/&nbsp
                <!-- <a href="resume/Sadashivaiah_CV_Academic.pdf" target="_blank">CV</a> &nbsp/&nbsp -->
                <a href="resume/Sadashivaiah_Resume.pdf" target="_blank">Resume</a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?user=lr78bS0AAAAJ&hl=en" target="_blank">Google Scholar</a> &nbsp/&nbsp
                <a href="https://twitter.com/vjysada" target="_blank">Twitter</a> &nbsp/&nbsp
                <a href="https://www.linkedin.com/in/vijaysadashivaiah/" target="_blank">LinkedIn</a> &nbsp/&nbsp
                <a href="https://github.com/vjysd" target="_blank"> GitHub </a>
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <img src="images/avatar.png" width="180" style="border-radius:50%" class="profile-image">
            </td>
          </tr>
        </tbody>
        </table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Recent News</heading>
              <p>
                <ul>
                  <li style="list-style-position:inside;margin:0;padding:0;text-align:left;">05/2024: I returned as a Research Intern at <a href="https://www.bosch-ai.com", target="_blank">Bosch Center for AI </a></li>
                  <li style="list-style-position:inside;margin:0;padding:0;text-align:left;">04/2024: I passed my candidacy exam to become a Ph.D. candidate</li>
                  <li style="list-style-position:inside;margin:0;padding:0;text-align:left;">04/2024: Our work on studing bias in vision language models was accepted to <a href="http://trustnlpworkshop.github.io", target="_blank">NAACL TrustNLP workshop</a></li>
                  <li style="list-style-position:inside;margin:0;padding:0;text-align:left;">01/2024: Our work on <a href="https://openreview.net/forum?id=BNP4MxzDEI", target="_blank">suppressing user determined semantic concepts in representation transfer</a> was accepted to <a href="https://jmlr.org/tmlr/", target="_blank">TMLR</a></li>
                  <li style="list-style-position:inside;margin:0;padding:0;text-align:left;">05/2023: Started as summer intern at <a href="https://www.bosch-ai.com", target="_blank">Bosch Center for AI </a></li>
                  <li style="list-style-position:inside;margin:0;padding:0;text-align:left;">01/2023: Our paper on spectral unmixing was published at <a href="https://bmcneurosci.biomedcentral.com/articles/10.1186/s12868-022-00765-1", target="_blank">BMC Neuroscience </a></li>
                  <li style="list-style-position:inside;margin:0;padding:0;text-align:left;">11/2022: I was a finalist at <a href="https://www.quadfellowship.org", target="_blank">Quad Fellowship </a></li>
                  <li style="list-style-position:inside;margin:0;padding:0;text-align:left;">10/2022: Our work on <a href="https://www.quadfellowship.org", target="_blank">Improving langugage model predictions using knowledge graphs</a> was accepted to DL4KG Workshop at <a href=https://iswc2022.semanticweb.org, target="_blank">ISWC-22</a></li>
                  <li style="list-style-position:inside;margin:0;padding:0;text-align:left;">07/2022: I was awarded a best poster award at <a href="http://2022.semanticwebschool.org", target="_blank">ISWS </a>summer school in Bertinaro, Italy</li>
                  <li style="list-style-position:inside;margin:0;padding:0;text-align:left;">04/2022: I was accepted to <a href="http://2022.semanticwebschool.org", target="_blank">ISWS </a>summer school in Bertinaro, Italy</li>
                  <li style="list-style-position:inside;margin:0;padding:0;text-align:left;">02/2022: Our work on <a href="https://arxiv.org/abs/2202.01011", target="_blank">improving transfer learning using multi-armed bandits</a> was accepted to <a href="https://www.iclr.cc/", target="_blank">ICLR-22</a></li>
                  <li style="list-style-position:inside;margin:0;padding:0;text-align:left;">01/2022: Started my research co-advised by <a href="https://scholar.google.com/citations?user=JNPbTdIAAAAJ&hl=en" target="_blank">Professor James A. Hendler </a> and <a href="https://scholar.google.com/citations?user=Z_rAu6sAAAAJ&hl=en" target="_blank">Professor Chris R. Sims</a></li>
                  <li style="list-style-position:inside;margin:0;padding:0;text-align:left;">05/2021: Started a summer project with <a href="https://research.ibm.com/", target="_blank">Trustworthy AI and RL groups at IBM Research</a></li>
                  <li style="list-style-position:inside;margin:0;padding:0;text-align:left;">01/2021: Started MS/PhD in Computer Science at <a href="https://cs.rpi.edu/", target="_blank">Rensselaer Polytechnic Institute</a></li>
                </ul>
              </p>
            </td>
          </tr>
        </tbody>
        </table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
          <tbody>
            <tr>
              <td style="padding:20px;width:100%;vertical-align:middle">
                <heading>Research</heading>
              </td>
            </tr>
          </tbody>
        </table>
        
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/miccai24.jpg" alt="PontTuset" width="200" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <papertitle>Explaining Chest X-ray Pathology Models using Textual Concepts</papertitle>
              <br>
              <strong>Vijay Sadashivaiah</strong>,
              <a href="https://www.massgeneral.org/doctors/17961/mannudeep-kalra" target="_blank">Mannudeep K Kalra</a>,
              <a href="https://researcher.watson.ibm.com/researcher/view.php?person=us-rluss" target="_blank">Ronny Luss</a>,
              <a href="https://dial.rpi.edu/people/pingkun-yan" target="_blank">Pingkun Yan</a>,
              <a href="https://faculty.rpi.edu/james-hendler" target="_blank">James A. Hendler</a>
              <br>
              AIM-FM at NeurIPS, 2024
              <br>
              <a href="https://arxiv.org/abs/2407.00557", target="_blank">paper</a>
              <p></p>
              <p></p>
              <p>
                We propose to explain chest X-ray pathology models using textual concepts. This is achieved by leveraging the joint latent space of image and text in vision-language models. 
              </p>
            </td>
          </tr>

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <img src="images/naacl24_trustnlp.png" alt="PontTuset" width="200" style="border-style: none">
              </td>
              <td width="75%" valign="middle">
                <papertitle>Beyond Visual Augmentation: Investigating Bias in Multi-Modal Text Generation</papertitle>
                <br>
              <a href="https://scholar.google.com/citations?user=-698GEMAAAAJ&hl=en" target="_blank">Fnu Mohbat</a>,
              <strong>Vijay Sadashivaiah</strong>,
              <a href="https://scholar.google.com/citations?user=-698GEMAAAAJ&hl=en" target="_blank">Keerthiram Murugesan</a>,
              <a href="https://researcher.watson.ibm.com/researcher/view.php?person=us-adhuran", target="_blank">Amit Dhurandhar</a>,
              <a href="https://researcher.watson.ibm.com/researcher/view.php?person=us-rluss" target="_blank">Ronny Luss</a>,
              <a href="https://sites.google.com/site/pinyuchenpage" target="_blank">Pin-Yu Chen</a>
              <br>
                TrustNLP at NAACL, 2024
                <br>
                <a href="files/naacl24_trustnlp.pdf", target="_blank">paper</a>
                <p></p>
                <p></p>
                <p>
                  We evaluated the influence of bias on multimodal text generation models. In particular, we studied the impact of visual augmentation using state-of-the-art diffusion models when generating text.
                </p>
              </td>
            </tr>
        
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/TMLRconcepts.png" alt="PontTuset" width="200" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <papertitle>To Transfer or Not to Transfer: Suppressing Concepts from Source Representations</papertitle>
              <br>
              <strong>Vijay Sadashivaiah*</strong>,
              <a href="https://scholar.google.com/citations?user=-698GEMAAAAJ&hl=en" target="_blank">Keerthiram Murugesan*</a>,
              <a href="https://researcher.watson.ibm.com/researcher/view.php?person=us-rluss" target="_blank">Ronny Luss</a>,
              <a href="https://sites.google.com/site/pinyuchenpage" target="_blank">Pin-Yu Chen</a>,
              <a href="https://faculty.rpi.edu/christopher-sims" target="_blank">Christopher R. Sims</a>,
              <a href="https://faculty.rpi.edu/james-hendler" target="_blank">James A. Hendler</a>,
              <a href="https://researcher.watson.ibm.com/researcher/view.php?person=us-adhuran", target="_blank">Amit Dhurandhar</a> (* equal contribution)
              <br>
              Transactions on Machine Learning Research (TMLR), 2024
              <br>
              <a href="https://openreview.net/pdf?id=BNP4MxzDEI", target="_blank">paper</a>
              <p></p>
              <p></p>
              <p>
                We propose to suppress user-determined semantically meaningful concepts (viz. eyeglasses, smiling) from intermediate representations in computer vision tasks. 
              </p>
            </td>
          </tr>
        
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/transferICLR22-gcam.png" alt="PontTuset" width="200" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <papertitle>Auto-Transfer: Learning to Route Transferrable Representations</papertitle>
              <br>
              <a href="https://scholar.google.com/citations?user=-698GEMAAAAJ&hl=en" target="_blank">Keerthiram Murugesan*</a>,
              <strong>Vijay Sadashivaiah*</strong>,
              <a href="https://researcher.watson.ibm.com/researcher/view.php?person=us-rluss" target="_blank">Ronny Luss</a>,
              <a href="https://sites.google.com/a/utexas.edu/karthiksh/", target="_blank">Karthikeyan Shanmugam</a>,
              <a href="https://sites.google.com/site/pinyuchenpage" target="_blank">Pin-Yu Chen</a>,
              <a href="https://researcher.watson.ibm.com/researcher/view.php?person=us-adhuran", target="_blank">Amit Dhurandhar</a> (* equal contribution)
              <br>
              International Conference on Learning Representations (ICLR), 2022
              <br>
              <a href="https://arxiv.org/abs/2202.01011", target="_blank">paper</a> /
              <a href="https://github.com/ibm/auto-transfer", target="_blank">code</a>
              <p></p>
              <p></p>
              <p>
                We introduce multi-armed bandit based representation routing to improve transfer learning in computer vision tasks. 
              </p>
            </td>
          </tr>

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <img src="images/DL4KG22.png" alt="PontTuset" width="200" style="border-style: none">
              </td>
              <td width="75%" valign="middle">
                <papertitle>Improving Language Model Predictions via Prompts Enriched with Knowledge Graphs</papertitle>
                <br>
                <a href="https://www.linkedin.com/in/ryan-brate-64532319b/?originalSubdomain=nl" target="_blank">Ryan Brate</a>,
                Minh-Hoang Dang,
                <a href="https://scholar.google.com/citations?user=8N_u7TcAAAAJ&hl=en" target="_blank">Fabian Hoppe</a>,
                <a href="https://www.linkedin.com/in/yuan-he-0557781aa/" target="_blank">Yuan He</a>,
                <a href="https://www.albertmeronyo.org" target="_blank">Albert Meroño-Peñuelar</a>,
                <strong>Vijay Sadashivaiah</strong>
                <br>
                Deep Learning for Knowledge Graphs Workshop at ISWC, 2022
                <br>
                <a href="https://hal.science/hal-03991267/document", target="_blank">paper</a>
                <p></p>
                <p></p>
                <p>
                  We propose to imporve language model predictions by enriching the prompts from knowledge graphs. 
                </p>
              </td>
            </tr>

            <tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <img src="images/sufi.jpg" alt="PontTuset" width="200" style="border-style: none">
              </td>
              <td width="75%" valign="middle">
                <papertitle>SUFI: An automated approach to spectral unmixing of fluorescent multiplex images captured in mouse and postmortem human brain tissues</papertitle>
                <br>
                <strong>Vijay Sadashivaiah</strong>,
                <a href="https://scholar.google.com/citations?user=TQGNTP8AAAAJ&hl=en" target="_blank">Madhavi Tippani</a>,
                <a href="https://www.libd.org/team/stephanie-cerceo-page-phd/" target="_blank">Stephanie C Page</a>,
                <a href="https://bcmb.bs.jhmi.edu/people/sang-ho-kwon/" target="_blank">Sang Ho Kwon</a>,
                <a href="https://scholar.google.com/citations?user=4EsDCWYAAAAJ&hl=en" target="_blank">Svitlana V Bach</a>,
                <a href="https://scholar.google.com/citations?user=Wvh7XEoAAAAJ&hl=en" target="_blank">Rahul A Bharadwaj</a>,
                <a href="https://www.libd.org/team/thomas-hyde/" target="_blank">Thomas M Hyde</a>,
                <a href="https://www.libd.org/team/joel-kleinman" target="_blank">Joel E Kleinman</a>,             
                <a href="http://aejaffe.com" target="_blank">Andrew E Jaffe</a>,
                <a href="https://www.libd.org/team/kristen-maynard-phd/" target="_blank">Kristen R Maynard</a>,
                <br>
                BMC Neuroscience, 2021
                <br>
                <a href="https://bmcneurosci.biomedcentral.com/articles/10.1186/s12868-022-00765-1", target="_blank">paper</a> /
                <a href="https://github.com/LieberInstitute/SUFI", target="_blank">code</a>
                <p></p>
                <p></p>
                <p>
                  An automated approach to spectral unmixing of fluorescent images
                </p>
              </td>
            </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/neuronTran21.jpeg" alt="PontTuset" width="200" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <papertitle>Single-nucleus transcriptome analysis reveals cell-type-specific molecular signatures across reward circuitry in the human brain</papertitle>
              <br>
              <a href="https://scholar.google.com/citations?user=BjitUH0AAAAJ&hl=en" target="_blank">Matthew N Tran</a>,
              <a href="https://www.libd.org/team/kristen-maynard-phd/" target="_blank">Kristen R Maynard</a>,
              <a href="https://scholar.google.com/citations?user=LIsY1vwAAAAJ&hl=en" target="_blank">Abby Spangler</a>,
              <a href="https://lahuuki.github.io" target="_blank">Louise A Huuki</a>,
              <a href="https://www.libd.org/team/kelsey-montgomery/" target="_blank">Kelsey D Montgomery</a>,
              <strong>Vijay Sadashivaiah</strong>,
              <a href="https://scholar.google.com/citations?user=TQGNTP8AAAAJ&hl=en" target="_blank">Madhavi Tippani</a>,
              <a href="https://scholar.google.com/citations?user=lv6jrnoAAAAJ&hl=en", target="_blank">Brianna K Barry</a>,
              <a href="https://www.rti.org/expert/dana-b-hancock" target="_blank">Dana B Hancock</a>,
              <a href="https://www.stephaniehicks.com", target="_blank">Stephanie C Hicks</a>,
              <a href="https://www.libd.org/team/joel-kleinman" target="_blank">Joel E Kleinman</a>,
              <a href="https://www.libd.org/team/thomas-hyde/" target="_blank">Thomas M Hyde</a>,
              <a href="https://lcolladotor.github.io" target="_blank">Leonardo Collado-Torres</a>,
              <a href="http://aejaffe.com" target="_blank">Andrew E Jaffe</a>,
              <a href="https://www.libd.org/team/keri-martinowich-phd/" target="_blank">Keri Martinowich</a>
              <br>
              Neuron 2021
              <br>
              <a href="https://www.sciencedirect.com/science/article/abs/pii/S0896627321006553", target="_blank">paper</a> /
              <a href="https://github.com/LieberInstitute/10xPilot_snRNAseq-human", target="_blank">code</a>
              <p></p>
              <p></p>
              <p>
                A single-nucleus RNA-sequencing resource of 70,615 high-quality nuclei to generate a molecular taxonomy of cell types across five human brain regions.
              </p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/kcnh2.png" alt="PontTuset" width="200" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <papertitle>KCNH2-3.1 mediates aberrant complement activation and impaired hippocampal-medial prefrontal circuitry associated with working memory deficits</papertitle>
              <br>
              Ming Ren,
              Zhonghua Hu, 
              <a href="https://scholar.google.com/citations?user=s22hIQ0AAAAJ&hl=en" target="_blank">Dr. Qiang Chen</a>, 
              <a href="http://aejaffe.com" target="_blank">Andrew E Jaffe</a>,
              Yingbo Li,
              <strong>Vijay Sadashivaiah</strong>,
              Shujuan Zhu, 
              Nina Rajpurohit, 
              Joo Heon Shin, Wei Xia, 
              Yankai Jia, 
              Jingxian Wu, 
              Sunny Lang Qin, 
              Xinjian Li, 
              Jian Zhu, 
              Qingjun Tian, 
              Daniel Paredes, 
              Fengyu Zhang, 
              Kuan Hong Wang, 
              <a href="https://scholar.google.com/citations?user=ox1LAuAAAAAJ&hl=en" target="_blank">Venkata S Mattay</a>, 
              Joseph H Callicott, 
              Karen F Berman, 
              Daniel R Weinberger, 
              Feng Yang
              <br>
              Molecular Psychiatry 2019
              <br>
              <a href="https://www.nature.com/articles/s41380-019-0530-1", target="_blank">paper</a>
              <p></p>
              <p></p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/nerveJCN2019.png" alt="PontTuset" width="200" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <papertitle>Modeling the interactions between stimulation and physiologically induced APs in a mammalian nerve fiber: dependence on frequency and fiber diameter.</papertitle>
              <br>
              <strong>Vijay Sadashivaiah</strong>,
              <a href="https://people.montefiore.uliege.be/sacre/" target="_blank">Pierre Sacré</a>,
              <a href="https://www.hopkinsmedicine.org/profiles/details/yun-guan" target="_blank">Yun Guan</a>,
              <a href="https://www.hopkinsmedicine.org/profiles/details/william-anderson" target="_blank">William S Anderson</a>,
              <a href="https://www.bme.jhu.edu/people/faculty/sridevi-v-sarma/" target="_blank">Sridevi V Sarma</a>
              <br>
              Journal of Computational Neuroscience 2019, EMBC 2018, EMBC 2017
              <br>
              <a href="https://link.springer.com/article/10.1007/s10827-018-0703-y", target="_blank">paper 1</a> /
              <a href="files/embc2018_1.pdf", target="_blank">paper 2</a> /
              <a href="files/embc2018_1.pdf", target="_blank">paper 3</a> /
              <a href="https://people.montefiore.uliege.be/sacre/pubs/Sadashivaiah2017aa.pdf", target="_blank">paper 4</a> /
              <a href="https://github.com/vjysd/nerve-fiber-modeling", target="_blank">code</a>
              <p></p>
              <p></p>
              <p>
                We constructed a mechanistic, stochastic and functional models of nerve fiber to quantify interactions.
              </p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/Neurophotonics2017.png" alt="PontTuset" width="200" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <papertitle>Voltage-sensitive dye imaging of mouse neocortex during a whisker detection task</papertitle>
              <br>
              <a href="https://www.researchgate.net/scientific-contributions/Alexandros-Kyriakatos-2034018601" target="_blank">Alexandros Kyriakatos</a>,
              <strong>Vijay Sadashivaiah</strong>,
              Yifei Zhang,
              Alessandro Motta,
              <a href="https://www.researchgate.net/profile/Matthieu-Auffret" target="_blank">Mattieu Auffret</a>,
              <a href="https://scholar.google.com/citations?user=rej0aokAAAAJ&hl=en" target="_blank">Carl CH Petersen</a>
              
              <br>
              Neurophotonics 2017
              <br>
              <a href="https://www.spiedigitallibrary.org/journals/neurophotonics/volume-4/issue-3/031204/Voltage-sensitive-dye-imaging-of-mouse-neocortex-during-a-whisker/10.1117/1.NPh.4.3.031204.full", target="_blank">paper</a> 
              <p></p>
              <p></p>
              <p>
                We studied the sensory motor interactions in mice brain using voltage sensitive dye imaging.
              </p>
            </td>
          </tr>

        </tbody>
        </table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:right;font-size:small;">
              Source code credit to <a href="https://jonbarron.info/" target="_blank">Dr. Jon Barron</a></p>
            </td>
          </tr>
        </tbody></table>
      </td>
    </tr>
  </table>
</body>

</html>
